{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moving-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime \n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-lyric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hearing-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices(device_type = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legislative-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "f16 = pd.read_csv('data/f16_l3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a49c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = f16['Force']\n",
    "y = f16['Acceleration3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc2818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = MinMaxScaler().fit_transform(X.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874b8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the number of model iterations we will run \n",
    "test_iterations = 4\n",
    "\n",
    "#Declare the size of the initial train test split\n",
    "test_size = 0.20\n",
    "\n",
    "#Declare the number of hidden layers to add\n",
    "layers = 4\n",
    "\n",
    "#Declare the density of the hidden layers\n",
    "density = 128\n",
    "\n",
    "#Declare the type of activation for the hidden layers\n",
    "activation = 'relu' \n",
    "#activation = ks.layers.LeakyReLU(alpha = 0.01)\n",
    "\n",
    "#Assign the loss function the model will use to train\n",
    "loss = 'mean_squared_error'\n",
    "\n",
    "#Assign the optimizer for the compilitation\n",
    "optimizer = 'adam'\n",
    "\n",
    "#Declare the patience count used for early stopping\n",
    "patience = 25\n",
    "\n",
    "#Declare the test size used for early stopping train test split\n",
    "es_test_size = 0.25\n",
    "\n",
    "#Declare the batch size for use in the model\n",
    "batch_size = 64\n",
    "\n",
    "#Declare the maximum number of epochs for our model\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df96ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_true, y_train, y_true = train_test_split(\n",
    "    X, y, random_state = 8669, test_size = test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bronze-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigns the computation to be performed via GPU Device:0\n",
    "with tf.device('/gpu:0'):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Creates a for loop that will add the number of layers based on the variable declared in the previous cell    \n",
    "    for i in range(layers):\n",
    "        #Adds a model layer with density and activation based on the variables declared in the previous cell\n",
    "        model.add(Dense(density, activation = activation))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    #model.compile(loss = loss, optimizer = optimizer, metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "detailed-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'loss', patience = patience, restore_best_weights = True)\n",
    "mc = ModelCheckpoint(filepath = 'test_model.h5', monitor = 'loss', save_best_only=True)\n",
    "X_es_train, X_es_true, y_es_train, y_es_true = train_test_split(X_train, y_train, test_size = es_test_size, random_state = 8669)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7cdf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataframe by which we will eventually put in our list created above\n",
    "model_record = pd.DataFrame(columns = ['model_num', 'loss_type', 'time', 'epochs', 'r2', 'mae', 'mse', 'rmse', 'max_error'])\n",
    "\n",
    "#Creates a dataframe by which our model's predicted values and true values will be stored\n",
    "predict_record = pd.DataFrame(y_true).reset_index(drop = True)\n",
    "\n",
    "#Creates a numpy array by which the for loop will use to count model runs and is then used to name df columns\n",
    "model_counter = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098002c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1006/1006 [==============================] - 4s 3ms/step - loss: 0.2328 - mse: 0.2328 - val_loss: 0.2385 - val_mse: 0.2385\n",
      "Epoch 2/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2286 - mse: 0.2286 - val_loss: 0.2382 - val_mse: 0.2382\n",
      "Epoch 3/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2276 - mse: 0.2276 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 4/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2266 - mse: 0.2266 - val_loss: 0.2343 - val_mse: 0.2343\n",
      "Epoch 5/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2338 - val_mse: 0.2338\n",
      "Epoch 6/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2239 - mse: 0.2239 - val_loss: 0.2387 - val_mse: 0.2387\n",
      "Epoch 7/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2214 - mse: 0.2214 - val_loss: 0.2301 - val_mse: 0.2301\n",
      "Epoch 8/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2192 - mse: 0.2192 - val_loss: 0.2262 - val_mse: 0.2262\n",
      "Epoch 9/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2171 - mse: 0.2171 - val_loss: 0.2290 - val_mse: 0.2290\n",
      "Epoch 10/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2162 - mse: 0.2162 - val_loss: 0.2214 - val_mse: 0.2214\n",
      "Epoch 11/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2153 - mse: 0.2153 - val_loss: 0.2257 - val_mse: 0.2257\n",
      "Epoch 12/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2145 - mse: 0.2145 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 13/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2135 - mse: 0.2135 - val_loss: 0.2198 - val_mse: 0.2198\n",
      "Epoch 14/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2120 - mse: 0.2120 - val_loss: 0.2257 - val_mse: 0.2257\n",
      "Epoch 15/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2111 - mse: 0.2111 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 16/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2117 - mse: 0.2117 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 17/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2120 - mse: 0.2120 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 18/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2124 - mse: 0.2124 - val_loss: 0.2312 - val_mse: 0.2312\n",
      "Epoch 19/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2118 - mse: 0.2118 - val_loss: 0.2208 - val_mse: 0.2208\n",
      "Epoch 20/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2133 - mse: 0.2133 - val_loss: 0.2249 - val_mse: 0.2249\n",
      "Epoch 21/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.2188 - val_mse: 0.2188\n",
      "Epoch 22/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2104 - mse: 0.2104 - val_loss: 0.2194 - val_mse: 0.2194\n",
      "Epoch 23/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2103 - mse: 0.2103 - val_loss: 0.2262 - val_mse: 0.2262\n",
      "Epoch 24/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2143 - mse: 0.2143 - val_loss: 0.2241 - val_mse: 0.2241\n",
      "Epoch 25/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2131 - mse: 0.2131 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 26/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2115 - mse: 0.2115 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 27/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2111 - mse: 0.2111 - val_loss: 0.2203 - val_mse: 0.2203\n",
      "Epoch 28/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2107 - mse: 0.2107 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 29/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2098 - mse: 0.2098 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 30/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2108 - mse: 0.2108 - val_loss: 0.2211 - val_mse: 0.2211\n",
      "Epoch 31/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2102 - mse: 0.2102 - val_loss: 0.2199 - val_mse: 0.2199\n",
      "Epoch 32/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2100 - mse: 0.2100 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 33/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2097 - mse: 0.2097 - val_loss: 0.2322 - val_mse: 0.2322\n",
      "Epoch 34/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2101 - mse: 0.2101 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 35/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2267 - val_mse: 0.2267\n",
      "Epoch 36/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2104 - mse: 0.2104 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 37/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 38/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2100 - mse: 0.2100 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 39/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2237 - val_mse: 0.2237\n",
      "Epoch 40/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2101 - mse: 0.2101 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 41/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2127 - mse: 0.2127 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 42/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2097 - mse: 0.2097 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 43/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2100 - mse: 0.2100 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 44/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2102 - mse: 0.2102 - val_loss: 0.2198 - val_mse: 0.2198\n",
      "Epoch 45/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 46/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2100 - mse: 0.2100 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 47/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 48/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2097 - mse: 0.2097 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 49/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 50/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 51/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2205 - val_mse: 0.2205\n",
      "Epoch 52/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2238 - val_mse: 0.2238\n",
      "Epoch 53/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2195 - val_mse: 0.2195\n",
      "Epoch 54/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2101 - mse: 0.2101 - val_loss: 0.2197 - val_mse: 0.2197\n",
      "Epoch 55/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2203 - val_mse: 0.2203\n",
      "Epoch 56/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2215 - val_mse: 0.2215\n",
      "Epoch 57/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2099 - mse: 0.2099 - val_loss: 0.2184 - val_mse: 0.2184\n",
      "Epoch 58/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2225 - val_mse: 0.2225\n",
      "Epoch 59/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2095 - mse: 0.2095 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 60/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2102 - mse: 0.2102 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 61/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2095 - mse: 0.2095 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 62/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 63/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 64/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 65/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 66/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2095 - mse: 0.2095 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 67/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2101 - mse: 0.2101 - val_loss: 0.2195 - val_mse: 0.2195\n",
      "Epoch 68/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2221 - val_mse: 0.2221\n",
      "Epoch 69/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 70/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 71/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2202 - val_mse: 0.2202\n",
      "Epoch 72/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 73/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2099 - mse: 0.2099 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 74/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2184 - val_mse: 0.2184\n",
      "Epoch 75/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 76/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 77/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 78/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2220 - val_mse: 0.2220\n",
      "Epoch 79/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 80/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2250 - val_mse: 0.2250\n",
      "Epoch 81/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 82/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 83/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 84/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 85/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2256 - val_mse: 0.2256\n",
      "Epoch 86/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 87/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 88/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2239 - val_mse: 0.2239\n",
      "Epoch 89/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2206 - val_mse: 0.2206\n",
      "Epoch 90/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 91/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 92/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 93/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 94/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 95/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 96/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 97/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2221 - val_mse: 0.2221\n",
      "Epoch 98/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 99/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 100/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2192 - val_mse: 0.2192\n",
      "Epoch 101/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 102/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2192 - val_mse: 0.2192\n",
      "Epoch 103/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 104/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 105/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2200 - val_mse: 0.2200\n",
      "Epoch 106/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 107/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2201 - val_mse: 0.2201\n",
      "Epoch 108/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 109/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 110/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 111/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 112/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 113/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 114/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 115/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 116/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 117/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 118/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 119/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2090 - mse: 0.2090 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 120/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 122/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 123/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 124/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 125/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 126/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2203 - val_mse: 0.2203\n",
      "Epoch 127/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 128/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2168 - val_mse: 0.2168\n",
      "Epoch 129/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 130/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 131/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 132/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 133/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 134/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2215 - val_mse: 0.2215\n",
      "Epoch 135/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 136/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 137/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 138/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 139/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2217 - val_mse: 0.2217\n",
      "Epoch 140/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 141/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2215 - val_mse: 0.2215\n",
      "Epoch 142/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2200 - val_mse: 0.2200\n",
      "Epoch 143/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 144/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 145/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 146/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 147/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 148/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 149/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2209 - val_mse: 0.2209\n",
      "Epoch 150/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 151/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 152/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 153/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 154/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 155/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 156/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 157/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2188 - val_mse: 0.2188\n",
      "Epoch 158/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 159/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 160/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 161/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 162/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 163/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 164/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 165/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 166/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 167/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2197 - val_mse: 0.2197\n",
      "Epoch 168/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 169/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 170/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 171/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 172/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 173/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 174/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2207 - val_mse: 0.2207\n",
      "Epoch 175/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 176/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 177/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 178/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2193 - val_mse: 0.2193\n",
      "Epoch 179/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 1/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2180 - val_mse: 0.2180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2168 - val_mse: 0.2168\n",
      "Epoch 3/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 4/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 5/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 6/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 7/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 8/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 9/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2194 - val_mse: 0.2194\n",
      "Epoch 10/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 11/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 12/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2192 - val_mse: 0.2192\n",
      "Epoch 13/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 14/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 15/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 16/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 17/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2168 - val_mse: 0.2168\n",
      "Epoch 18/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 19/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 20/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 21/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2078 - mse: 0.2078 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 22/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 23/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 24/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 25/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 26/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 27/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2198 - val_mse: 0.2198\n",
      "Epoch 28/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2188 - val_mse: 0.2188\n",
      "Epoch 29/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 30/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2214 - val_mse: 0.2214\n",
      "Epoch 31/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2167 - val_mse: 0.2167\n",
      "Epoch 32/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 33/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 34/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 35/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2237 - val_mse: 0.2237\n",
      "Epoch 36/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 37/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 38/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 39/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 40/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 41/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 42/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 43/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 44/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2193 - val_mse: 0.2193\n",
      "Epoch 45/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2192 - val_mse: 0.2192\n",
      "Epoch 46/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2198 - val_mse: 0.2198\n",
      "Epoch 1/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 2/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 3/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 4/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 5/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2200 - val_mse: 0.2200\n",
      "Epoch 6/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 7/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2354 - val_mse: 0.2354\n",
      "Epoch 8/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 9/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 10/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2078 - mse: 0.2078 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 11/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 12/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 13/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 14/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 15/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 16/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 17/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 18/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 19/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 20/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 21/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 22/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2199 - val_mse: 0.2199\n",
      "Epoch 23/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 24/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 25/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 26/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 27/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 28/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 29/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 30/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 31/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 32/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 33/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 34/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 35/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 1/250\n",
      "1006/1006 [==============================] - 4s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2184 - val_mse: 0.2184\n",
      "Epoch 2/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 3/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2087 - mse: 0.2087 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 4/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 5/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 6/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2184 - val_mse: 0.2184\n",
      "Epoch 7/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 8/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2085 - mse: 0.2085 - val_loss: 0.2194 - val_mse: 0.2194\n",
      "Epoch 9/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 10/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2174 - val_mse: 0.2174\n",
      "Epoch 11/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2212 - val_mse: 0.2212\n",
      "Epoch 12/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 13/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 14/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 15/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 16/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 17/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 18/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 19/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 20/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 21/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 22/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 23/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 24/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 25/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 26/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2180 - val_mse: 0.2180\n",
      "Epoch 27/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 28/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 29/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 30/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2200 - val_mse: 0.2200\n",
      "Epoch 31/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 32/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2168 - val_mse: 0.2168\n",
      "Epoch 33/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2204 - val_mse: 0.2204\n",
      "Epoch 34/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2175 - val_mse: 0.2175\n",
      "Epoch 35/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 36/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2179 - val_mse: 0.2179\n",
      "Epoch 37/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2181 - val_mse: 0.2181\n",
      "Epoch 38/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 39/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 40/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 41/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 42/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 43/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 44/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2083 - mse: 0.2083 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 45/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2189 - val_mse: 0.2189\n",
      "Epoch 46/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 47/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2167 - val_mse: 0.2167\n",
      "Epoch 48/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 49/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2168 - val_mse: 0.2168\n",
      "Epoch 50/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2082 - mse: 0.2082 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 51/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2245 - val_mse: 0.2245\n",
      "Epoch 52/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 53/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.2173 - val_mse: 0.2173\n",
      "Epoch 54/250\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.2206 - val_mse: 0.2206\n"
     ]
    }
   ],
   "source": [
    "#Assigns the computation to be performed via GPU Device:0\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "#Performs model training repeatedly based on the variable declared previously    \n",
    "    for i in range(test_iterations):\n",
    "        model.compile(loss = loss, optimizer = optimizer, metrics = ['mse'])\n",
    "\n",
    "        #Creates an empty list for storing model metrics and other information \n",
    "        record_list = list() \n",
    "        #Starts a counter that adds 1 everytime a fitting is performed\n",
    "        model_counter = model_counter + 1\n",
    "        #Starts a timer to end after the round of fitting is complete\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        #Fits our model/batch_size and epochs are declared previously\n",
    "        #model.fit(x = X_train, y = y_train.values, \n",
    "        #          batch_size = batch_size, epochs = epochs)\n",
    "        \n",
    "        history = model.fit(x = X_es_train, y = y_es_train.values, \n",
    "                  validation_data = (X_es_true, y_es_true),\n",
    "                  batch_size = batch_size, epochs = epochs, \n",
    "                  callbacks = [es, mc])\n",
    "        \n",
    "        #Saves our model predictions\n",
    "        y_pred = model.predict(X_true)\n",
    "        \n",
    "        #Saves our model run #, time to run, and model metrics to our temporary list\n",
    "        record_list.extend([len(model_record)+1, #A model iteration count\n",
    "                            loss, #Records the loss metric used to fit the model\n",
    "                            format(datetime.now() - start_time), #Calculates the time it took to complete the model\n",
    "                            len(history.history['loss']), #The epoch number of our model checkpoint \n",
    "                            r2_score(y_true, y_pred), #R2\n",
    "                            mean_absolute_error(y_true, y_pred), #MAE\n",
    "                            mean_squared_error(y_true, y_pred), #MSE\n",
    "                            np.sqrt(mean_squared_error(y_true, y_pred)), #RMSE\n",
    "                            max_error(y_true, y_pred) #MaxError\n",
    "                           ])        \n",
    "        \n",
    "        #Adds the temporary list of model metrics (etc) to the end of a dataframe\n",
    "        model_record.loc[len(model_record)] = record_list\n",
    "        \n",
    "        #Converts our predictions to a dataframe so it will play nice\n",
    "        y_pred_df = pd.DataFrame(y_pred)\n",
    "        \n",
    "        #Adds predictions as a column to the end of a dataframe and names is accordingly\n",
    "        predict_record = pd.concat([predict_record, y_pred_df], axis = 1)\n",
    "        predict_record = predict_record.rename(columns = {0 : 'm' + str(model_counter[0])})\n",
    "        \n",
    "        #Calculates the residual values for each prediction and stores it as a dataframe \n",
    "        residuals_df = pd.DataFrame(abs(predict_record.iloc[:,len(predict_record.columns)-1] - predict_record.iloc[:,0]))\n",
    "        \n",
    "        #Adds y residuals as a column to the end of a dataframe and names is accordingly\n",
    "        predict_record = pd.concat([predict_record, residuals_df], axis = 1)\n",
    "        predict_record = predict_record.rename(columns = {0 : 'res' + str(model_counter[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affa6aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_num</th>\n",
       "      <th>loss_type</th>\n",
       "      <th>time</th>\n",
       "      <th>epochs</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>max_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0:09:10.544527</td>\n",
       "      <td>179</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.229131</td>\n",
       "      <td>0.217333</td>\n",
       "      <td>0.466190</td>\n",
       "      <td>5.226147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0:02:21.821995</td>\n",
       "      <td>46</td>\n",
       "      <td>0.189253</td>\n",
       "      <td>0.229513</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>0.465514</td>\n",
       "      <td>5.231953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0:01:48.031540</td>\n",
       "      <td>35</td>\n",
       "      <td>0.188763</td>\n",
       "      <td>0.228867</td>\n",
       "      <td>0.216835</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>5.231925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0:02:46.536850</td>\n",
       "      <td>54</td>\n",
       "      <td>0.192455</td>\n",
       "      <td>0.230446</td>\n",
       "      <td>0.215848</td>\n",
       "      <td>0.464594</td>\n",
       "      <td>5.231256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_num           loss_type            time epochs        r2       mae  \\\n",
       "0         1  mean_squared_error  0:09:10.544527    179  0.186899  0.229131   \n",
       "1         2  mean_squared_error  0:02:21.821995     46  0.189253  0.229513   \n",
       "2         3  mean_squared_error  0:01:48.031540     35  0.188763  0.228867   \n",
       "3         4  mean_squared_error  0:02:46.536850     54  0.192455  0.230446   \n",
       "\n",
       "        mse      rmse  max_error  \n",
       "0  0.217333  0.466190   5.226147  \n",
       "1  0.216704  0.465514   5.231953  \n",
       "2  0.216835  0.465655   5.231925  \n",
       "3  0.215848  0.464594   5.231256  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "welsh-posting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acceleration3</th>\n",
       "      <th>m1</th>\n",
       "      <th>res1</th>\n",
       "      <th>m2</th>\n",
       "      <th>res2</th>\n",
       "      <th>m3</th>\n",
       "      <th>res3</th>\n",
       "      <th>m4</th>\n",
       "      <th>res4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.314890</td>\n",
       "      <td>-0.098931</td>\n",
       "      <td>0.215959</td>\n",
       "      <td>-0.075298</td>\n",
       "      <td>0.239592</td>\n",
       "      <td>-0.053967</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>-0.105321</td>\n",
       "      <td>0.209569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.267150</td>\n",
       "      <td>-0.276631</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>-0.281761</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>-0.272139</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>-0.295437</td>\n",
       "      <td>0.028287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257710</td>\n",
       "      <td>0.254854</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.246601</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>0.280225</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.323467</td>\n",
       "      <td>0.065757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.193330</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>0.093559</td>\n",
       "      <td>-0.107190</td>\n",
       "      <td>0.086140</td>\n",
       "      <td>-0.106385</td>\n",
       "      <td>0.086945</td>\n",
       "      <td>-0.105088</td>\n",
       "      <td>0.088242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.409560</td>\n",
       "      <td>-0.120076</td>\n",
       "      <td>0.289484</td>\n",
       "      <td>-0.122826</td>\n",
       "      <td>0.286734</td>\n",
       "      <td>-0.120458</td>\n",
       "      <td>0.289102</td>\n",
       "      <td>-0.135839</td>\n",
       "      <td>0.273721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21446</th>\n",
       "      <td>-0.021237</td>\n",
       "      <td>-0.086881</td>\n",
       "      <td>0.065644</td>\n",
       "      <td>-0.097871</td>\n",
       "      <td>0.076634</td>\n",
       "      <td>-0.094188</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>-0.104534</td>\n",
       "      <td>0.083297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21447</th>\n",
       "      <td>-0.264320</td>\n",
       "      <td>0.098779</td>\n",
       "      <td>0.363099</td>\n",
       "      <td>0.086443</td>\n",
       "      <td>0.350763</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>0.361760</td>\n",
       "      <td>0.074308</td>\n",
       "      <td>0.338628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21448</th>\n",
       "      <td>0.060178</td>\n",
       "      <td>-0.068791</td>\n",
       "      <td>0.128969</td>\n",
       "      <td>-0.064325</td>\n",
       "      <td>0.124503</td>\n",
       "      <td>-0.038473</td>\n",
       "      <td>0.098651</td>\n",
       "      <td>-0.106361</td>\n",
       "      <td>0.166539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21449</th>\n",
       "      <td>0.535320</td>\n",
       "      <td>0.240120</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.210879</td>\n",
       "      <td>0.324441</td>\n",
       "      <td>0.241959</td>\n",
       "      <td>0.293361</td>\n",
       "      <td>0.277449</td>\n",
       "      <td>0.257871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21450</th>\n",
       "      <td>0.036212</td>\n",
       "      <td>0.059046</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.060664</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Acceleration3        m1      res1        m2      res2        m3  \\\n",
       "0          -0.314890 -0.098931  0.215959 -0.075298  0.239592 -0.053967   \n",
       "1          -0.267150 -0.276631  0.009481 -0.281761  0.014611 -0.272139   \n",
       "2           0.257710  0.254854  0.002856  0.246601  0.011109  0.280225   \n",
       "3          -0.193330 -0.099771  0.093559 -0.107190  0.086140 -0.106385   \n",
       "4          -0.409560 -0.120076  0.289484 -0.122826  0.286734 -0.120458   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "21446      -0.021237 -0.086881  0.065644 -0.097871  0.076634 -0.094188   \n",
       "21447      -0.264320  0.098779  0.363099  0.086443  0.350763  0.097440   \n",
       "21448       0.060178 -0.068791  0.128969 -0.064325  0.124503 -0.038473   \n",
       "21449       0.535320  0.240120  0.295200  0.210879  0.324441  0.241959   \n",
       "21450       0.036212  0.059046  0.022834  0.047086  0.010874  0.060664   \n",
       "\n",
       "           res3        m4      res4  \n",
       "0      0.260923 -0.105321  0.209569  \n",
       "1      0.004989 -0.295437  0.028287  \n",
       "2      0.022515  0.323467  0.065757  \n",
       "3      0.086945 -0.105088  0.088242  \n",
       "4      0.289102 -0.135839  0.273721  \n",
       "...         ...       ...       ...  \n",
       "21446  0.072951 -0.104534  0.083297  \n",
       "21447  0.361760  0.074308  0.338628  \n",
       "21448  0.098651 -0.106361  0.166539  \n",
       "21449  0.293361  0.277449  0.257871  \n",
       "21450  0.024452  0.036474  0.000262  \n",
       "\n",
       "[21451 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce845589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 49,921\n",
      "Trainable params: 49,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Nifty call to confirm our variables were properly inputted into our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4067b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_record.to_csv('data/rnn_results_1.csv')\n",
    "#predict_record.to_csv('data/rnn_predictions_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
